Objective: Develop a reinforcement learning (RL) model for cryptocurrency trading that aims to achieve a profit target of 2% to 4% per trade. The model should utilize a 1-minute time interval for data analysis and trading decisions, with built-in continuous learning capabilities.

Data Features
The model will use the following features, derived from the cryptocurrency market data:

Raw Features:
Timestamp: The time at which the data point is recorded.
Open: The opening price of the cryptocurrency for the minute.
High: The highest price during the minute.
Low: The lowest price during the minute.
Close: The closing price of the cryptocurrency for the minute.
Volume: The total trading volume during the minute.
Derived Features:
Price Change: The difference between the current close and the previous close.
Percentage Change: The percentage change from the previous close to the current close.
Moving Averages: Calculate the Simple Moving Average (SMA) and Exponential Moving Average (EMA) for different periods (e.g., 5, 10, 20 minutes).
Volatility: Calculate the standard deviation of the closing prices over a specified window (e.g., 5 minutes).
Relative Strength Index (RSI): Calculate the RSI to identify overbought or oversold conditions.
Bollinger Bands: Calculate the upper and lower bands based on the moving average and standard deviation.
Order Book Imbalance: If available, derive features from the order book data, such as the difference between buy and sell orders.
Model Architecture
Reinforcement Learning Algorithm:
Use Deep Q-Networks (DQN) or Proximal Policy Optimization (PPO) as the base algorithm.
Implement Experience Replay to store past experiences and sample them during training.
Utilize Target Networks to stabilize learning.
Policy Gradient Methods:
Consider using policy gradient methods to directly optimize the trading policy.
Exploration Strategies:
Implement an epsilon-greedy strategy for exploration, gradually decreasing epsilon over time to favor exploitation as the model learns.
Reward Structure:
Define the reward function to provide positive rewards for achieving the profit target (2% to 4%) and negative rewards for losses or failing to meet the target.
Hierarchical Reinforcement Learning:
Break down the trading task into sub-tasks, such as entry, exit, and risk management, allowing for more efficient learning.
Continuous Learning and Online Learning Techniques
Online Learning Framework:
Integrate an online learning framework that allows the model to update its parameters incrementally as new data arrives. This can be achieved by:
Continuously feeding the model with the latest market data at 1-minute intervals.
Updating the model weights based on new experiences without retraining from scratch.
Adaptive Learning Rate:
Implement an adaptive learning rate that adjusts based on the performance of the model. This can help the model learn more effectively from new data while avoiding drastic changes that could destabilize learning.
Incremental Experience Replay:
Maintain a dynamic replay buffer that prioritizes recent experiences while still retaining older experiences. This allows the model to learn from both recent market conditions and historical data.
Model Evaluation and Adjustment:
Regularly evaluate the model's performance on new data and adjust the learning strategy as needed. This could involve retraining certain components of the model or fine-tuning hyperparameters based on recent performance metrics.
Training Process
Data Preprocessing:
Normalize the features to ensure they are on a similar scale.
Split the data into training, validation, and test sets.
Training Loop:
For each episode, initialize the environment with the current market state.
For each time step, select an action (buy, sell, hold) based on the current policy.
Execute the action and observe the new state and reward.
Store the experience in the replay buffer and update the model using sampled experiences.
Evaluation:
After training, evaluate the model on unseen data to assess its performance in achieving the profit target.
Monitor metrics such as total profit, win rate, and maximum drawdown.
Additional Considerations
**Risk

After finished please give a detailed step by step instructions on how to use this program.